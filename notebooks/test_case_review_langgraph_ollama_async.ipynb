{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Run your LangGraph workflow with one compiled graph per Ollama port, in parallel.\n",
    "\n",
    "Prereqs:\n",
    "  pip install langgraph langchain langchain-community langchain-ollama pandas openpyxl\n",
    "\n",
    "Start multiple Ollama instances (examples):\n",
    "  ollama serve -p 11434 &\n",
    "  ollama serve -p 11435 &\n",
    "  ollama serve -p 11436 &\n",
    "  ollama serve -p 11437 &\n",
    "  ollama serve -p 11438 &\n",
    "Ensure the model (e.g., \"llama3.1\") is pulled on each instance.\n",
    "\n",
    "Author: Daniel-ready multiport runner\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Annotated, List, TypedDict, Dict, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# LangGraph core\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "\n",
    "# Messages (LangChain core)\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ChatOllama integration (prefer the dedicated package, fallback to community)\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama  # preferred\n",
    "except Exception:\n",
    "    from langchain_community.chat_models import ChatOllama  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1) State definition (adds \"env\" since nodes read it)\n",
    "# --------------------------------------------------------------------\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List, add_messages]    # chat history, aggregated via add_messages\n",
    "    structure: str\n",
    "    objective: str\n",
    "    testcases: str\n",
    "    review_summary: str\n",
    "    env: Dict[str, Any]                        # optional env payload (e.g., port, model, etc.)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2) NODES (YOUR EXACT NODE FUNCTIONS)\n",
    "#     NOTE: They rely on a module-level `llm` which will be set per process.\n",
    "# --------------------------------------------------------------------\n",
    "# Global `llm` placeholder; each worker process will assign it.\n",
    "llm = None\n",
    "\n",
    "def eval_structure(state: State) -> Dict[str, Any]:\n",
    "    testcases = state.get(\"testcases\", \"\")\n",
    "    environment = state.get(\"env\", {})\n",
    "\n",
    "    system_message = \"\"\"You are a Senior Software QA Architect. Your task is to evaluate the structure of a test case against the provided Acceptance Criteria. You will determine whether the acceptance criteria has been appropriately satisfied:\n",
    "    Acceptance Criteria:\n",
    "    - Test case steps follow a logical, sequential path that ensures reproducibility.\n",
    "    - Actions are documented where data collection is required, and data collection is accurate and complete.\n",
    "    - Steps are clear, concise, and free of jargon; absolutes are avoided.\n",
    "    - Steps are numbered or ordered for easy execution.\n",
    "    - Outcome aligns with the expected results.\n",
    "\n",
    "    You will provide: assessment_verdict, assessment_rationale, identified_gaps, actionable_recommendations, and test_case_improvements. Propose improvements detecting misalignment between test case objective and actual steps.\n",
    "\n",
    "    Response Format (produce exactly this JSON structure):\n",
    "    {\n",
    "    \"assessment_verdict\": \"complete|partial|inadequate\",\n",
    "    \"assessment_rationale\": \"<description of how and why the value for assessment_verdict was chosen>\",\n",
    "    \"identified_gaps\": [\"<gap 1>\", \"<gap 2>\", ...],\n",
    "    \"recommendations\": [\"<recommendation 1>\", \"<recommendation 2>\", ...],\n",
    "    \"test_case_improvements\": [\"<improvement 1>\", \"<improvement 2>\", ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"Task: Evaluate the completeness of a test case against the acceptance criteria for Test Case Structure.\n",
    "    Input Variables:\n",
    "    - Test Case:\n",
    "    {testcases}\n",
    "    Produce output strictly in the Response Format JSON. Do not use Markdown.\n",
    "\n",
    "    Now perform the review on the provided Input Variables and return only the Response Format JSON.\n",
    "    \"\"\"\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=user_message)\n",
    "    ])\n",
    "\n",
    "    ai_msg = AIMessage(content=resp.content, name=\"eval_structure\")\n",
    "    updates = {\n",
    "        \"messages\": [ai_msg],\n",
    "        \"structure\": resp.content,\n",
    "    }\n",
    "    return updates\n",
    "\n",
    "def eval_objective(state: State) -> Dict[str, Any]:\n",
    "    testcases = state.get(\"testcases\", \"\")\n",
    "    environment = state.get(\"env\", {})\n",
    "\n",
    "    system_message = \"\"\"You are a Senior Software QA Architect. Your task is to evaluate the completeness of a test case against the provided Acceptance Criteria. You will determine whether the acceptance criteria has been appropriately satisfied:\n",
    "    Acceptance Criteria:\n",
    "        - Test case includes all required components: unique ID, descriptive title, preconditions, input data, expected results, and detailed steps.\n",
    "        - Test case objective is clear, specific, and aligned with the requirement.\n",
    "        - Test case meets its intended objective (e.g., verifies actual outcome, not just UI interaction).\n",
    "        - Both positive and negative scenarios are included.\n",
    "        - Expected results align with the requirement and provide sufficient evidence for verification.\n",
    "        - Completeness score should meet or exceed 80%.\n",
    "\n",
    "    You will provide: assessment_verdict, assessment_rationale, identified_gaps, actionable_recommendations, and test_case_improvements. Propose improvements detecting misalignment between test case objective and actual steps.\n",
    "\n",
    "    Response Format (produce exactly this JSON structure):\n",
    "    {\n",
    "        \"assessment_verdict\": \"complete|partial|inadequate\",\n",
    "        \"assessment_rationale\": \"<description of how and why the value for assessment_verdict was chosen>\",\n",
    "        \"identified_gaps\": [\"<gap 1>\", \"<gap 2>\", ...],\n",
    "        \"recommendations\": [\"<recommendation 1>\", \"<recommendation 2>\", ...],\n",
    "        \"test_case_improvements\": [\"<improvement 1>\", \"<improvement 2>\", ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"\n",
    "    Task: Evaluate the completeness of a test case against the acceptance criteria for Test Case Completeness.\n",
    "    Input Variables:\n",
    "    - Test Case:\n",
    "    {testcases}\n",
    "\n",
    "    Acceptance Criteria:\n",
    "    - Test case includes all required components: unique ID, descriptive title, preconditions, input data, expected results, and detailed steps.\n",
    "    - Test case objective is clear, specific, and aligned with the requirement.\n",
    "    - Test case meets its intended objective (e.g., verifies actual outcome, not just UI interaction).\n",
    "    - Both positive and negative scenarios are included.\n",
    "    - Expected results align with the requirement and provide sufficient evidence for verification.\n",
    "    - Completeness score should meet or exceed 80%.\n",
    "\n",
    "    Produce output strictly in the Response Format JSON. Do not use Markdown.\n",
    "    \"\"\"\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=user_message)\n",
    "    ])\n",
    "\n",
    "    ai_msg = AIMessage(content=resp.content, name=\"eval_objective\")\n",
    "    updates = {\n",
    "        \"messages\": [ai_msg],\n",
    "        \"objective\": resp.content,\n",
    "    }\n",
    "    return updates\n",
    "\n",
    "def get_review_summary(state: State) -> Dict[str, Any]:\n",
    "    environment = state.get(\"env\", {})\n",
    "\n",
    "    all_messages_text = []\n",
    "    for m in state.get(\"messages\", []):\n",
    "        content = getattr(m, \"content\", str(m))\n",
    "        all_messages_text.append(content)\n",
    "\n",
    "    system_message = \"\"\"\n",
    "    You are a Senior Test Verification Traceability Analyst with expertise in software quality assurance.\n",
    "    You specialize in summarizing test case reviews, ensuring that critical improvements and recommendations are captured to provide a robust summary rationale.\n",
    "\n",
    "    Response Format (produce exactly this JSON structure):\n",
    "\n",
    "    {\n",
    "        'testcase_review_summary': <summary of the outputs from test case review nodes>\n",
    "    }\n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"\n",
    "    Summarize the below test case review:\n",
    "\n",
    "    ## Inputs\n",
    "    Test Case Review: \n",
    "    {json.dumps(all_messages_text, ensure_ascii=False)}\n",
    "\n",
    "    ## Notes \n",
    "    Produce output strictly in the described Response Format\n",
    "    \"\"\"\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=user_message)\n",
    "    ])\n",
    "\n",
    "    ai_msg = AIMessage(content=resp.content, name=\"get_review_summary\")\n",
    "    updates = {\n",
    "        \"messages\": [ai_msg],\n",
    "        \"review_summary\": resp.content\n",
    "    }\n",
    "\n",
    "    return updates\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3) Graph builder (compile with your nodes)\n",
    "# --------------------------------------------------------------------\n",
    "def compile_workflow() -> Any:\n",
    "    graph = StateGraph(State)\n",
    "    graph.add_node(\"eval_structure\", eval_structure)\n",
    "    graph.add_node(\"eval_objective\", eval_objective)\n",
    "    graph.add_node(\"get_review_summary\", get_review_summary)\n",
    "\n",
    "    graph.add_edge(START, \"eval_structure\")\n",
    "    graph.add_edge(START, \"eval_objective\")\n",
    "    graph.add_edge(\"eval_structure\", \"get_review_summary\")\n",
    "    graph.add_edge(\"eval_objective\", \"get_review_summary\")\n",
    "    graph.add_edge(\"get_review_summary\", END)\n",
    "\n",
    "    return graph.compile()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4) Multiport runner class\n",
    "# --------------------------------------------------------------------\n",
    "class LangGraphMultiPort:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_df: pd.DataFrame,\n",
    "        prompt_vars: List[str] = None,        # columns to include; must include 'testcases'\n",
    "        model: str = \"llama3.1\",\n",
    "        start_port: int = 11434,\n",
    "        num_ports: int = 5,\n",
    "        model_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        progress_every: int = 10,\n",
    "        output_path: Optional[str] = \"./output/langgraph_results.xlsx\",\n",
    "        enable_json_flatten: bool = True,     # flatten review_summary JSON if parsable\n",
    "        json_key: Optional[str] = None,\n",
    "        env_payload: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        if prompt_vars is None:\n",
    "            prompt_vars = input_df.columns.tolist()\n",
    "        if \"testcases\" not in prompt_vars:\n",
    "            raise ValueError(\"prompt_vars must include the 'testcases' column for these nodes.\")\n",
    "\n",
    "        self.df = input_df\n",
    "        self.prompt_vars = prompt_vars\n",
    "        self.model = model\n",
    "        self.start_port = start_port\n",
    "        self.num_ports = num_ports\n",
    "        self.model_kwargs = model_kwargs or {\"temperature\": 0.1, \"format\": \"json\", \"keep_alive\": \"30m\"}\n",
    "        self.progress_every = progress_every\n",
    "        self.output_path = output_path\n",
    "        self.enable_json_flatten = enable_json_flatten\n",
    "        self.json_key = json_key\n",
    "        self.env_payload = env_payload or {}\n",
    "\n",
    "        # Prepare records with original index\n",
    "        self.records = []\n",
    "        for i, row in self.df.iterrows():\n",
    "            rec = {k: row[k] for k in self.prompt_vars}\n",
    "            rec[\"_index\"] = i\n",
    "            self.records.append(rec)\n",
    "\n",
    "        self.ports = [self.start_port + i for i in range(self.num_ports)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _chunk(items: List[Dict[str, Any]], n_chunks: int) -> List[List[Dict[str, Any]]]:\n",
    "        if n_chunks <= 0:\n",
    "            return [items]\n",
    "        size = math.ceil(len(items) / n_chunks)\n",
    "        return [items[i:i + size] for i in range(0, len(items), size)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_state(item: Dict[str, Any], env: Dict[str, Any]) -> State:\n",
    "        return State(\n",
    "            messages=[],\n",
    "            structure=\"\",\n",
    "            objective=\"\",\n",
    "            testcases=str(item.get(\"testcases\", \"\")),\n",
    "            review_summary=\"\",\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _flatten_json(text: str, json_key: Optional[str]) -> Dict[str, Any]:\n",
    "        out: Dict[str, Any] = {}\n",
    "        try:\n",
    "            data = json.loads(text)\n",
    "            payload = data\n",
    "            if json_key and isinstance(data, dict) and json_key in data:\n",
    "                payload = data[json_key]\n",
    "            if isinstance(payload, dict):\n",
    "                for k, v in payload.items():\n",
    "                    out[f\"summary.{k}\"] = v\n",
    "            elif isinstance(payload, list):\n",
    "                for i, d in enumerate(payload):\n",
    "                    if isinstance(d, dict):\n",
    "                        for k, v in d.items():\n",
    "                            out[f\"summary[{i}].{k}\"] = v\n",
    "                    else:\n",
    "                        out[f\"summary[{i}]\"] = d\n",
    "        except Exception:\n",
    "            pass\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _worker(args) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Worker for one port. Sets the global llm, compiles the workflow, and processes items.\n",
    "        \"\"\"\n",
    "        (port, model, model_kwargs, items, progress_every, enable_json_flatten, json_key, base_env) = args\n",
    "\n",
    "        # Assign per-process llm (so node functions use the right instance)\n",
    "        globals()['llm'] = ChatOllama(model=model, base_url=f\"http://localhost:{port}\", **(model_kwargs or {}))\n",
    "\n",
    "        workflow = compile_workflow()\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        total = len(items)\n",
    "        start = time.time()\n",
    "        print(f\"[Port {port}] Processing {total} items...\")\n",
    "\n",
    "        for idx, item in enumerate(items, start=1):\n",
    "            try:\n",
    "                env = dict(base_env)\n",
    "                env.update({\"port\": port, \"model\": model})\n",
    "                state = LangGraphMultiPort._prepare_state(item, env)\n",
    "                out_state: State = workflow.invoke(state)  # sync nodes\n",
    "\n",
    "                row = {\n",
    "                    \"port\": port,\n",
    "                    \"item_index\": item.get(\"_index\", idx - 1),\n",
    "                    \"structure\": out_state.get(\"structure\", \"\"),\n",
    "                    \"objective\": out_state.get(\"objective\", \"\"),\n",
    "                    \"review_summary\": out_state.get(\"review_summary\", \"\"),\n",
    "                    \"testcases\": out_state.get(\"testcases\", \"\"),\n",
    "                }\n",
    "\n",
    "                if enable_json_flatten:\n",
    "                    # Flatten review_summary only (you can also parse structure/objective if desired)\n",
    "                    row.update(LangGraphMultiPort._flatten_json(row[\"review_summary\"], json_key))\n",
    "\n",
    "                results.append(row)\n",
    "\n",
    "                if progress_every and (idx % progress_every == 0 or idx == total):\n",
    "                    elapsed = time.time() - start\n",
    "                    print(f\"[Port {port}] {idx}/{total} done ({elapsed:.1f}s)\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Port {port}] ERROR on item idx={idx}: {e}\\n{traceback.format_exc()}\")\n",
    "                results.append({\n",
    "                    \"port\": port,\n",
    "                    \"item_index\": item.get(\"_index\", idx - 1),\n",
    "                    \"error\": str(e),\n",
    "                    \"testcases\": str(item.get(\"testcases\", \"\")),\n",
    "                })\n",
    "\n",
    "        print(f\"[Port {port}] Completed {total} items in {time.time()-start:.1f}s\")\n",
    "        return results\n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        # Split items across ports\n",
    "        chunks = self._chunk(self.records, len(self.ports))\n",
    "\n",
    "        # Build worker args\n",
    "        worker_args = []\n",
    "        for port, items in zip(self.ports, chunks):\n",
    "            worker_args.append((\n",
    "                port,\n",
    "                self.model,\n",
    "                self.model_kwargs,\n",
    "                items,\n",
    "                self.progress_every,\n",
    "                self.enable_json_flatten,\n",
    "                self.json_key,\n",
    "                self.env_payload\n",
    "            ))\n",
    "\n",
    "        print(f\"Spawning {len(worker_args)} workers for ports: {self.ports}\")\n",
    "        all_results: List[Dict[str, Any]] = []\n",
    "\n",
    "        from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "        with ProcessPoolExecutor(max_workers=len(worker_args)) as ex:\n",
    "            futs = [ex.submit(LangGraphMultiPort._worker, args) for args in worker_args]\n",
    "            for fut in as_completed(futs):\n",
    "                res = fut.result()\n",
    "                all_results.extend(res)\n",
    "\n",
    "        # Sort and save\n",
    "        all_results.sort(key=lambda r: r.get(\"item_index\", 0))\n",
    "        df_out = pd.DataFrame(all_results)\n",
    "\n",
    "        if self.output_path:\n",
    "            Path(self.output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            df_out.to_excel(self.output_path, index=False)\n",
    "            print(f\"Results written to: {self.output_path}\")\n",
    "\n",
    "        return df_out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spawning 5 workers for ports: [11436, 11437, 11438, 11439, 11440]\n",
      "[Port 11440] Processing 1 items...[Port 11436] Processing 1 items...[Port 11437] Processing 1 items...[Port 11438] Processing 1 items...[Port 11439] Processing 1 items...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Port 11438] 1/1 done (80.6s)\n",
      "[Port 11438] Completed 1 items in 80.6s\n",
      "[Port 11439] 1/1 done (115.5s)\n",
      "[Port 11439] Completed 1 items in 115.5s\n",
      "[Port 11437] 1/1 done (132.9s)\n",
      "[Port 11437] Completed 1 items in 132.9s\n",
      "[Port 11440] 1/1 done (284.3s)\n",
      "[Port 11440] Completed 1 items in 284.3s\n",
      "[Port 11436] 1/1 done (303.9s)\n",
      "[Port 11436] Completed 1 items in 303.9s\n",
      "Results written to: output/test_case_review_results_test_dec_8_2025.xlsx\n",
      "Processed 5 rows in 304.1s\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 5) End-to-end usage\n",
    "# --------------------------------------------------------------------\n",
    "# Adjust to your file path\n",
    "input_excel = \"./ready_for_testcase_summarization_prompt_test.xlsx\"\n",
    "if not os.path.exists(input_excel):\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_excel}\")\n",
    "\n",
    "df = pd.read_excel(input_excel, engine=\"openpyxl\")\n",
    "prompt_vars = [\"testcases\"]  # minimal set needed by your nodes\n",
    "\n",
    "# Ports + model (mirrors the batch style)\n",
    "num_ports = 5\n",
    "start_port = 11436\n",
    "model = \"deepseek-r1\" #\"llama3.1\"\n",
    "model_kwargs = {\"temperature\": 0.1, \"format\": \"json\", \"keep_alive\": \"30m\"}\n",
    "\n",
    "output_dir = Path(\"./output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = str(output_dir / \"test_case_review_results_test_dec_8_2025.xlsx\")\n",
    "\n",
    "runner = LangGraphMultiPort(\n",
    "    input_df=df,\n",
    "    prompt_vars=prompt_vars,\n",
    "    model=model,\n",
    "    start_port=start_port,\n",
    "    num_ports=num_ports,\n",
    "    model_kwargs=model_kwargs,\n",
    "    progress_every=10,\n",
    "    output_path=output_path,\n",
    "    enable_json_flatten=True,     # set False if you prefer raw JSON strings\n",
    "    json_key=None,                # e.g., 'testcase_review_summary' to extract only that\n",
    "    env_payload={\"run_id\": \"structure_objective_summary_v1\"}  # optional\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "df_out = runner.run()\n",
    "print(f\"Processed {len(df_out)} rows in {time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_traceability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
