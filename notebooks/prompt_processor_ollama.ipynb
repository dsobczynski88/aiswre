{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import flatdict\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional, Union, Tuple\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from string import Formatter\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updated PromptProcessor (LLama)\n",
    "\n",
    "\"\"\"\n",
    "A utility for processing natural language prompts in parallel using multiple Ollama instances.\n",
    "\n",
    "This class handles the end-to-end workflow of:\n",
    "1. Loading input data from various file formats (CSV, Excel, JSON)\n",
    "2. Formatting prompts with variables from the input data\n",
    "3. Distributing prompt processing across multiple Ollama instances\n",
    "4. Tracking progress with visual indicators\n",
    "5. Handling errors and retries\n",
    "6. Processing and flattening JSON responses\n",
    "7. Saving results to Excel files\n",
    "\n",
    "The processor supports optional RAG (Retrieval Augmented Generation) functionality\n",
    "by incorporating relevant context from PDF documents when generating responses.\n",
    "\n",
    "Example usage:\n",
    "    processor = PromptProcessor(\n",
    "        input_file=\"./data.xlsx\",\n",
    "        output_dir=\"./results\",\n",
    "        model=\"llama3.1\",\n",
    "        model_kwargs={\"temperature\": 0.3}\n",
    "    )\n",
    "    \n",
    "    results = await processor.run_prompt_batch(\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "        user_message_template=\"Analyze this data: {data}\",\n",
    "        prompt_name=\"data-analysis\",\n",
    "        items=items,\n",
    "        ids=ids\n",
    "    )\n",
    "\"\"\"\n",
    "    \n",
    "class PromptProcessor:\n",
    "    \"\"\"Process prompts in parallel using multiple Ollama instances.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                input_file: Optional[str] = None, \n",
    "                output_dir: str = \"./output\", \n",
    "                model: str = \"llama3\", \n",
    "                pdf_directory: Optional[str] = None, \n",
    "                use_rag: bool = False,\n",
    "                input_df: Optional[pd.DataFrame] = None,\n",
    "                model_kwargs: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the prompt processor.\n",
    "        \n",
    "        Args:\n",
    "            input_file: Path to the input file (CSV, Excel, etc.)\n",
    "            output_dir: Directory to save output results\n",
    "            model: LLM model to use for processing\n",
    "            pdf_directory: Directory containing PDF files for RAG (optional)\n",
    "            use_rag: Whether to use RAG functionality\n",
    "            input_df: Optional dataframe to use directly instead of loading from file\n",
    "            model_kwargs: Additional kwargs for the LLM model\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If neither input_file nor input_df is provided\n",
    "        \"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir\n",
    "        self.model = model\n",
    "        self.pdf_directory = pdf_directory\n",
    "        self.rag_processor = None\n",
    "        self.input_df = input_df\n",
    "        self.model_kwargs = model_kwargs or {\n",
    "            \"temperature\": 0.3,\n",
    "            \"format\": \"json\",\n",
    "            \"keep_alive\": \"1h\"\n",
    "        }\n",
    "        self.last_port = None\n",
    "        # Initialize RAG if requested\n",
    "        if use_rag:\n",
    "            self.rag_processor = RAGProcessor(pdf_directory, model)\n",
    "            \n",
    "        # Load dataframe if input_file is provided and input_df is not\n",
    "        if input_file and input_df is None:\n",
    "            self.load_input_data()\n",
    "        elif input_df is not None:\n",
    "            self.input_df = input_df\n",
    "        elif not input_file and input_df is None:\n",
    "            raise ValueError(\"Either input_file or input_df must be provided\")\n",
    "\n",
    "    def df_to_prompt_items(self, df: pd.DataFrame, columns: List[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Convert dataframe rows to a list of dictionaries for prompt variables.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            columns: List of column names to include (None for all columns)\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries, each representing variables for a prompt\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If specified columns don't exist in the dataframe\n",
    "        \"\"\"\n",
    "        if columns is None:\n",
    "            columns = df.columns.tolist()\n",
    "        else:\n",
    "            # Ensure all specified columns exist in the dataframe\n",
    "            missing = [col for col in columns if col not in df.columns]\n",
    "            if missing:\n",
    "                raise ValueError(f\"Columns not found in dataframe: {missing}\")\n",
    "        \n",
    "        # Convert each row to a dictionary with only the specified columns\n",
    "        return df[columns].to_dict(orient='records')\n",
    "        \n",
    "    async def process_json_responses(self, \n",
    "                              responses: List[Dict], \n",
    "                              ids: List[Any], \n",
    "                              prompt_type: str, \n",
    "                              json_key: str = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process responses and flatten extracted JSON structures.\n",
    "        \n",
    "        Args:\n",
    "            responses: List of responses from the LLM\n",
    "            ids: List of identifiers corresponding to each response\n",
    "            prompt_type: Type of prompt used (for tracking)\n",
    "            json_key: Optional key to extract from JSON response\n",
    "            \n",
    "        Returns:\n",
    "            List of processed and flattened dictionaries\n",
    "        \"\"\"\n",
    "        processed = []\n",
    "\n",
    "        for i, response in enumerate(responses):\n",
    "            output = {}\n",
    "            \n",
    "            # Handle None responses (failed prompts)\n",
    "            if response is None:\n",
    "                output = {\n",
    "                    \"item_id\": ids[i],\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"error\": \"Prompt failed after retry\"\n",
    "                }\n",
    "                processed.append(output)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Extract content from response\n",
    "                if isinstance(response, str):\n",
    "                    content = response\n",
    "                elif isinstance(response, dict) and \"response\" in response:\n",
    "                    content = response[\"response\"].content\n",
    "                else:\n",
    "                    content = str(response)\n",
    "                    \n",
    "                try:\n",
    "                    response_json = json.loads(content)\n",
    "                    if json_key and json_key in response_json:\n",
    "                        nested_dicts = response_json[json_key]\n",
    "                        if isinstance(nested_dicts, list):\n",
    "                            flat_dicts = [flatdict.FlatDict(d, delimiter=\".\") for d in nested_dicts]\n",
    "                            for d in flat_dicts:\n",
    "                                output.update(d)\n",
    "                        elif isinstance(nested_dicts, dict):\n",
    "                            flat_dict = flatdict.FlatDict(nested_dicts, delimiter=\".\")\n",
    "                            output.update(flat_dict)\n",
    "                    else:\n",
    "                        # If no json_key specified or not found, use the whole response\n",
    "                        flat_dict = flatdict.FlatDict(response_json, delimiter=\".\")\n",
    "                        output.update(flat_dict)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    output[\"json_parse_error\"] = content\n",
    "            except Exception as e:\n",
    "                output[\"processing_error\"] = str(e)\n",
    "                output[\"raw_response\"] = str(response)\n",
    "                \n",
    "            # Add metadata\n",
    "            output.update({\n",
    "                \"item_id\": ids[i],\n",
    "                \"prompt_type\": prompt_type,\n",
    "            })\n",
    "            \n",
    "            processed.append(output)\n",
    "        return processed\n",
    "        \n",
    "    ## Updated run_prompt_batch method\n",
    "    async def run_prompt_batch(self, \n",
    "                llm,  # Original parameter kept for compatibility\n",
    "                system_message: str, \n",
    "                user_message_template: str, \n",
    "                prompt_name: str, \n",
    "                items: List[Dict[str, Any]], \n",
    "                ids: List[Any] = None, \n",
    "                json_key: str = None,\n",
    "                start_port: int = 11434,\n",
    "                num_ports: int = 1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Execute prompts in parallel using multiple Ollama instances with progress tracking.\n",
    "        \n",
    "        Args:\n",
    "            llm: Ignored (kept for compatibility)\n",
    "            system_message: System message for the LLM\n",
    "            user_message_template: Template with {variable} placeholders\n",
    "            prompt_name: Name of the prompt for tracking\n",
    "            items: List of dictionaries with template variables\n",
    "            ids: Optional identifiers for each item\n",
    "            json_key: Optional key to extract from JSON response\n",
    "            num_ports: Number of Ollama instances to use\n",
    "            \n",
    "        Returns:\n",
    "            List of processed responses\n",
    "        \"\"\"\n",
    "        # Use sequential IDs if none provided\n",
    "        if ids is None:\n",
    "            ids = list(range(len(items)))\n",
    "        \n",
    "        # Get retriever for RAG if enabled\n",
    "        retriever = self.rag_processor.get_retriever() if self.rag_processor else None\n",
    "        \n",
    "        # Format all user messages\n",
    "        formatted_messages = []\n",
    "        for item in items:\n",
    "            user_msg = user_message_template\n",
    "            for key, value in item.items():\n",
    "                placeholder = f\"{{{key}}}\"\n",
    "                if placeholder in user_msg:\n",
    "                    user_msg = user_msg.replace(placeholder, str(value))\n",
    "            formatted_messages.append(user_msg)\n",
    "        \n",
    "        # Configure multiple Ollama instances\n",
    "        PORTS=[]\n",
    "        port_range=list(np.arange(0, num_ports, 1))\n",
    "        for p in port_range:\n",
    "            PORTS.append(start_port+p)\n",
    "        self.last_port = PORTS[-1]\n",
    "        \n",
    "        models = [\n",
    "            ChatOllama(\n",
    "                model=self.model,\n",
    "                base_url=f\"http://localhost:{port}\",\n",
    "                **self.model_kwargs\n",
    "            )\n",
    "            for port in PORTS\n",
    "        ]\n",
    "        \n",
    "        # Create a shared counter for overall progress\n",
    "        total_messages = len(formatted_messages)\n",
    "        processed_count = 0\n",
    "        \n",
    "        # Create a lock for updating the counter\n",
    "        counter_lock = asyncio.Lock()\n",
    "        \n",
    "        async def process_message(model, message):\n",
    "            \"\"\"Process a single message with retry logic\"\"\"\n",
    "            nonlocal processed_count\n",
    "            \n",
    "            # Try with one retry on failure\n",
    "            for attempt in range(2):\n",
    "                try:\n",
    "                    # Prepare question with RAG context if needed\n",
    "                    question = message\n",
    "                    if retriever:\n",
    "                        docs = retriever.invoke(message)\n",
    "                        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "                        question = f\"Context information:\\n{context}\\n\\nUser query:\\n{message}\"\n",
    "                    \n",
    "                    # Use SystemMessage and HumanMessage directly to avoid template issues\n",
    "                    result = await model.ainvoke([\n",
    "                        SystemMessage(content=system_message),\n",
    "                        HumanMessage(content=question)\n",
    "                    ])\n",
    "                    \n",
    "                    # Update the counter\n",
    "                    async with counter_lock:\n",
    "                        processed_count += 1\n",
    "                        \n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    if attempt == 0:\n",
    "                        print(f\"Error: {str(e)}. Retrying...\")\n",
    "                    else:\n",
    "                        print(f\"Retry failed. Skipping this message.\")\n",
    "                        # Update the counter even for failed messages\n",
    "                        async with counter_lock:\n",
    "                            processed_count += 1\n",
    "                        return {}\n",
    "        \n",
    "        async def process_distributed(messages, models):\n",
    "            \"\"\"Distribute messages across available models with progress tracking\"\"\"\n",
    "            # Calculate chunk size for each model\n",
    "            num_models = len(models)\n",
    "            try:\n",
    "                # Original calculation that might cause ZeroDivisionError\n",
    "                chunk_size = (len(messages) + num_models - 1) // num_models\n",
    "            except ZeroDivisionError:\n",
    "                # If num_models is 0 or division error occurs, set chunk_size to handle all messages\n",
    "                raise\n",
    "            \n",
    "            # Create chunks of messages\n",
    "            chunks = [\n",
    "                messages[i:i + chunk_size] \n",
    "                for i in range(0, len(messages), chunk_size)\n",
    "            ]\n",
    "            \n",
    "            # Create the main progress bar\n",
    "            main_progress = tqdm_asyncio(\n",
    "                total=total_messages,\n",
    "                desc=\"Overall Progress\",\n",
    "                position=0,\n",
    "                leave=True\n",
    "            )\n",
    "            \n",
    "            # Process each chunk with a dedicated model\n",
    "            async def process_chunk(model, chunk):\n",
    "                results = []\n",
    "                for msg in chunk:\n",
    "                    result = await process_message(model, msg)\n",
    "                    results.append(result)\n",
    "                    # Update the main progress bar\n",
    "                    main_progress.update(1)\n",
    "                return results\n",
    "            \n",
    "            # Run all chunks in parallel\n",
    "            print(f\"Processing {len(messages)} messages using {len(models)} Ollama instances...\")\n",
    "            results_nested = await asyncio.gather(*[\n",
    "                process_chunk(models[i], chunks[i]) \n",
    "                for i in range(min(len(chunks), len(models)))\n",
    "            ])\n",
    "            \n",
    "            # Close the progress bar\n",
    "            main_progress.close()\n",
    "            \n",
    "            # Flatten results\n",
    "            return [item for sublist in results_nested for item in sublist]\n",
    "        \n",
    "        # Process all messages in parallel with progress tracking\n",
    "        start_time = time.time()\n",
    "        responses = await process_distributed(formatted_messages, models)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Processed {len(responses)} messages in {elapsed:.2f}s\")\n",
    "        \n",
    "        # Prepare results with IDs\n",
    "        result_items = []\n",
    "        for item_id, response in zip(ids, responses):\n",
    "            if response is not None:\n",
    "                result_items.append({\n",
    "                    \"id\": item_id,\n",
    "                    \"response\": response,\n",
    "                    \"prompt_name\": prompt_name\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Item with ID {item_id} failed to process\")\n",
    "        \n",
    "        # Process JSON responses\n",
    "        return await self.process_json_responses(result_items, ids, prompt_name, json_key)\n",
    "\n",
    "\n",
    "## main function\n",
    "\n",
    "async def main(\n",
    "    system_message: str = \"Act as a helpful assistant\",\n",
    "    system_message_file: str = None,\n",
    "    user_template: str = None,\n",
    "    user_template_file: str = None,\n",
    "    input_file: str = None,\n",
    "    input_df: pd.DataFrame = None,\n",
    "    prompt_name: str = \"default-prompt\",\n",
    "    num_ports: int = 1,\n",
    "    start_port: int = 11434,\n",
    "    prompt_vars: List[str] = None,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    json_key: str = None,\n",
    "    output_dir: str = \"./output\",\n",
    "    output_filename: str = \"results.xlsx\",\n",
    "    model: str = \"llama3.1\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Process prompts with the specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        system_message: System message for the LLM\n",
    "        system_message_file: Path to file containing system message (overrides system_message if provided)\n",
    "        user_template: User message template\n",
    "        user_template_file: Path to file containing user template (overrides user_template if provided)\n",
    "        input_file: Path to input file (CSV, Excel, etc.)\n",
    "        input_df: DataFrame to use directly instead of loading from file\n",
    "        prompt_name: Name of the prompt for tracking\n",
    "        num_ports: Number of Ollama instances to use\n",
    "        prompt_vars: List of column names to include as variables\n",
    "        model_kwargs: Additional kwargs for the LLM model\n",
    "        json_key: Optional key to extract from JSON response\n",
    "        output_dir: Directory to save output results\n",
    "        output_filename: Name of the output file\n",
    "        model: LLM model to use for processing\n",
    "    \n",
    "    Returns:\n",
    "        List of processed responses\n",
    "    \"\"\"\n",
    "    # Load system message from file if provided\n",
    "    if system_message_file:\n",
    "        try:\n",
    "            with open(system_message_file, 'r') as f:\n",
    "                system_message = f.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading system message from file: {str(e)}\")\n",
    "            print(f\"Using default system message instead.\")\n",
    "    \n",
    "    # Load user template from file if provided\n",
    "    if user_template_file:\n",
    "        try:\n",
    "            with open(user_template_file, 'r') as f:\n",
    "                user_template = f.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading user template from file: {str(e)}\")\n",
    "            if not user_template:\n",
    "                raise ValueError(\"No user template provided and failed to load from file.\")\n",
    "    elif not user_template:\n",
    "        raise ValueError(\"Either user_template or user_template_file must be provided.\")\n",
    "    \n",
    "    # Set default model kwargs if not provided\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = {\n",
    "            \"temperature\": 0.3,\n",
    "            \"format\": \"json\",\n",
    "            \"keep_alive\": \"1h\"\n",
    "        }\n",
    "    \n",
    "    # Initialize the processor\n",
    "    processor = PromptProcessor(\n",
    "        input_file=input_file,\n",
    "        output_dir=output_dir,\n",
    "        model=model,\n",
    "        input_df=input_df,\n",
    "        model_kwargs=model_kwargs\n",
    "    )\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare the items and IDs\n",
    "    if prompt_vars is None:\n",
    "        prompt_vars = processor.input_df.columns.tolist()\n",
    "    \n",
    "    print(processor.input_df)\n",
    "    items = processor.df_to_prompt_items(processor.input_df, prompt_vars)\n",
    "    \n",
    "    # Use documentKey as ID if available, otherwise use index\n",
    "    if \"documentKey\" in processor.input_df.columns:\n",
    "        ids = processor.input_df[\"documentKey\"].tolist()\n",
    "    else:\n",
    "        ids = list(range(len(processor.input_df)))\n",
    "\n",
    "    # Create a placeholder LLM (will be replaced in run_prompt_batch)\n",
    "    placeholder_llm = None\n",
    "\n",
    "    # Process the batch directly\n",
    "    print(f\"Starting analysis with prompt: {prompt_name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call run_prompt_batch directly\n",
    "    results = await processor.run_prompt_batch(\n",
    "        llm=placeholder_llm,\n",
    "        system_message=system_message,\n",
    "        user_message_template=user_template,\n",
    "        prompt_name=prompt_name,\n",
    "        items=items,\n",
    "        ids=ids,\n",
    "        json_key=json_key,\n",
    "        num_ports=num_ports,\n",
    "        start_port=start_port\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Processed {len(results)} items in {elapsed:.2f}s\")\n",
    "\n",
    "    # Save the results\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        results_df.to_excel(output_path, index=False)\n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        \n",
    "        # Print a sample of the results\n",
    "        print(\"\\nSample results:\")\n",
    "        sample_cols = min(5, len(results_df.columns))\n",
    "        for col in results_df.columns[:sample_cols]:\n",
    "            print(f\"{col}: {results_df[col].iloc[0]}\")\n",
    "    else:\n",
    "        print(\"No results were generated\")\n",
    "\n",
    "    return results, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_message(input_source, find_message_type):\n",
    "    \n",
    "    if find_message_type not in [\"system\", \"user\"]:\n",
    "        raise ValueError(\"find_message_type must be either 'system' or 'user'\") \n",
    "    \n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try to open as a file\n",
    "        with open(input_source, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    except (IOError, TypeError):\n",
    "        # If it's not a valid file path, assume it's already a string\n",
    "        text = input_source\n",
    "        \n",
    "    if find_message_type == \"system\":\n",
    "        # Extract text between \"SYSTEM MESSAGE:\" and \"USER MESSAGE:\"\n",
    "        system_start = text.find(\"SYSTEM MESSAGE:\")\n",
    "        user_start = text.find(\"USER MESSAGE:\")\n",
    "        if system_start == -1 or user_start == -1:\n",
    "            return \"\"\n",
    "        # Add length of \"SYSTEM MESSAGE:\" to get the actual start of content\n",
    "        system_start += len(\"SYSTEM MESSAGE:\")\n",
    "        return text[system_start:user_start].strip()\n",
    "    else:  # find_message_type == \"user\"\n",
    "        # Extract text from \"USER MESSAGE:\" to the end\n",
    "        user_start = text.find(\"USER MESSAGE:\")\n",
    "        if user_start == -1:\n",
    "            return \"\"\n",
    "        # Add length of \"USER MESSAGE:\" to get the actual start of content\n",
    "        user_start += len(\"USER MESSAGE:\")\n",
    "        return text[user_start:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example 3: Loading templates from files and using a DataFrame directly\n",
    "df = pd.read_excel(\"./ready_for_testcase_summarization_prompt.xlsx\")\n",
    "\n",
    "prompt_template_fp = Path(\"../src/data/test_case_structure_prompt.txt\")\n",
    "\n",
    "num_ports = 5\n",
    "prompt_name = prompt_template_fp.stem \n",
    "system_message = extract_message(prompt_template_fp, \"system\")\n",
    "user_template = extract_message(prompt_template_fp, \"user\")\n",
    "prompt_vars = [\"testcases\"]\n",
    "output_filename = f\"{prompt_name}_results.xlsx\"\n",
    "model_kwargs = {\"temperature\": 0.1, \"format\": \"json\", \"keep_alive\": \"30m\"}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a Senior Software QA Architect. Your task is to evaluate the structure of a test case against the provided Acceptance Criteria. You will determine whether the acceptance criteria has been appropriately satisfied:\\n\\nAcceptance Criteria:\\n    - Test case steps follow a logical, sequential path that ensures reproducibility.\\n    - Actions are documented where data collection is required, and data collection is accurate and complete.\\n    - Steps are clear, concise, and free of jargon; absolutes are avoided.\\n    - Steps are numbered or ordered for easy execution.\\n    - Outcome aligns with the expected results.\\n\\nYou will provide: assessment_verdict, assessment_rationale, identified_gaps, actionable_recommendations, and test_case_improvements. Propose improvements detecting misalignment between test case objective and actual steps.\\n\\nResponse Format (produce exactly this JSON structure):\\n{\\n    \"assessment_verdict\": \"complete|partial|inadequate\",\\n    \"assessment_rationale\": \"<description of how and why the value for assessment_verdict was chosen>\",\\n    \"identified_gaps\": [\"<gap 1>\", \"<gap 2>\", ...],\\n    \"recommendations\": [\"<recommendation 1>\", \"<recommendation 2>\", ...],\\n    \"test_case_improvements\": [\"<improvement 1>\", \"<improvement 2>\", ...]\\n}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task: Evaluate the completeness of a test case against the acceptance criteria for Test Case Structure.\\n\\n## Input Variables:\\n- Test Case:\\n{testcases}\\n\\nAcceptance Criteria:\\n- Test case steps follow a logical, sequential path that ensures reproducibility.\\n- Actions are documented where data collection is required, and data collection is accurate and complete.\\n- Steps are clear, concise, and free of jargon; absolutes are avoided.\\n- Steps are numbered or ordered for easy execution.\\n- Outcome aligns with the expected path defined in the requirement.\\n\\nProduce output strictly in the Response Format JSON. Do not use Markdown.\\n\\nNow perform the review on the provided Input Variables and return only the Response Format JSON.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0     documentKey  \\\n",
      "0             5    P1320-TEST-2   \n",
      "1             6  P1320-TEST-202   \n",
      "2             7  P1320-TEST-203   \n",
      "3             8  P1320-TEST-204   \n",
      "4             9  P1320-TEST-206   \n",
      "..          ...             ...   \n",
      "193         198   P1320-TEST-67   \n",
      "194         199   P1320-TEST-69   \n",
      "195         200   P1320-TEST-70   \n",
      "196         201   P1320-TEST-73   \n",
      "197         202   P1320-TEST-95   \n",
      "\n",
      "                                           description  \\\n",
      "0     this test case verifies within the local moni...   \n",
      "1     obsoleting this since it is already covered i...   \n",
      "2     this test case verifies within the local moni...   \n",
      "3     this test case verifies within the local moni...   \n",
      "4     this test case verifies within the local moni...   \n",
      "..                                                 ...   \n",
      "193   the anchor shall have labeling which displays...   \n",
      "194   this test case verifies that on the exterior ...   \n",
      "195   the staff and asset shall have labeling which...   \n",
      "196   the charging station shall have labeling appl...   \n",
      "197   this test case verifies that all of the softw...   \n",
      "\n",
      "                                              setup$71  \\\n",
      "0     data sets modules local monitor login as [log...   \n",
      "1     follow the steps from [setup] [steps] [15] an...   \n",
      "2     data sets modules local monitor login as [log...   \n",
      "3     data sets modules local monitor login as [log...   \n",
      "4     data sets modules local monitor login as [log...   \n",
      "..                                                 ...   \n",
      "193   test datasets and modules hardware device as ...   \n",
      "194   test datasets and modules hardware device as ...   \n",
      "195   test datasets and modules hardware device as ...   \n",
      "196   test datasets and modules hardware device as ...   \n",
      "197   data sets modules standard software environme...   \n",
      "\n",
      "                                             all_steps  \\\n",
      "0    Step: 1.  to verify the control available to c...   \n",
      "1    Step: 1.  follow the steps from [server info] ...   \n",
      "2    Step: 1.  to verify the display of local monit...   \n",
      "3    Step: 1.  to verify the control available to c...   \n",
      "4    Step: 1.  to verify the search filter will be ...   \n",
      "..                                                 ...   \n",
      "193  Step: 1.  to verify the product label displays...   \n",
      "194  Step: 1.  to verify the company branding is vi...   \n",
      "195  Step: 1.  to verify the product label displays...   \n",
      "196  Step: 1.  to verify the product label displays...   \n",
      "197  Step: 1.  to verify the company branding and l...   \n",
      "\n",
      "                                   all_expectedResults  \\\n",
      "0    ExpectedResult: 1.  the create group shall be ...   \n",
      "1    ExpectedResult: 1.  1 system shall upgrade suc...   \n",
      "2    ExpectedResult: 1.  a list of all existing tag...   \n",
      "3    ExpectedResult: 1.  create tag control shall b...   \n",
      "4    ExpectedResult: 1.  search filter shall be ava...   \n",
      "..                                                 ...   \n",
      "193  ExpectedResult: 1.  anchor label shall display...   \n",
      "194  ExpectedResult: 1.  company brand should be vi...   \n",
      "195  ExpectedResult: 1.  staff tag label shall disp...   \n",
      "196  ExpectedResult: 1.  each product variation lab...   \n",
      "197  ExpectedResult: 1.  baxter branding and logo s...   \n",
      "\n",
      "                                             testcases  \n",
      "0    Test Case ID: P1320-TEST-2\\nTest Case Objectiv...  \n",
      "1    Test Case ID: P1320-TEST-202\\nTest Case Object...  \n",
      "2    Test Case ID: P1320-TEST-203\\nTest Case Object...  \n",
      "3    Test Case ID: P1320-TEST-204\\nTest Case Object...  \n",
      "4    Test Case ID: P1320-TEST-206\\nTest Case Object...  \n",
      "..                                                 ...  \n",
      "193  Test Case ID: P1320-TEST-67\\nTest Case Objecti...  \n",
      "194  Test Case ID: P1320-TEST-69\\nTest Case Objecti...  \n",
      "195  Test Case ID: P1320-TEST-70\\nTest Case Objecti...  \n",
      "196  Test Case ID: P1320-TEST-73\\nTest Case Objecti...  \n",
      "197  Test Case ID: P1320-TEST-95\\nTest Case Objecti...  \n",
      "\n",
      "[198 rows x 7 columns]\n",
      "Starting analysis with prompt: test_case_structure_prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 198 messages using 5 Ollama instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 198/198 [11:40<00:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 198 messages in 700.83s\n",
      "Processed 198 items in 700.84s\n",
      "Results saved to: ./output/test_case_structure_prompt_results.xlsx\n",
      "\n",
      "Sample results:\n",
      "assessment_verdict: partial\n",
      "assessment_rationale: The test case steps follow a logical path, but some actions are not clearly documented where data collection is required. Additionally, some expected results contain absolutes (e.g., 'the create group control shall change to enabled state'). The outcome generally aligns with the expected path defined in the requirement.\n",
      "identified_gaps: ['Actions for data collection are not clearly documented', 'Expected results contain absolutes']\n",
      "actionable_recommendations: ['Clearly document actions where data collection is required', 'Rephrase expected results to avoid absolutes']\n",
      "test_case_improvements: ['Consider breaking down long steps into smaller, more manageable tasks', 'Use a consistent format for documenting expected results']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, processor = asyncio.run(main(\n",
    "    system_message=system_message,\n",
    "    user_template=user_template,\n",
    "    input_df=df,\n",
    "    prompt_name=prompt_name,\n",
    "    num_ports=num_ports,\n",
    "    prompt_vars=prompt_vars,\n",
    "    model_kwargs=model_kwargs,\n",
    "    output_filename=output_filename\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11435"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.last_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('output/testcase_objective_review.xlsx'), PosixPath('output/testcase_summaries.xlsx'), PosixPath('output/testcase_structure_prompt_results.xlsx')]\n"
     ]
    }
   ],
   "source": [
    "from src import utils\n",
    "\n",
    "all_results_df = utils.concat_matching_dataframes(\n",
    "    _path=\"./output\",                     # base directory to scan\n",
    "    _regex=rf\"testcase_.*.xlsx$\",             # regex applied to filenames\n",
    "    recursive=False,\n",
    "    case_sensitive=True,\n",
    "    match_on=\"name\",\n",
    "    read_kwargs=None,\n",
    "    ignore_index=True,\n",
    "    check_list_like_columns=False,\n",
    "    axis=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assessment_verdict</th>\n",
       "      <th>assessment_rationale</th>\n",
       "      <th>identified_gaps</th>\n",
       "      <th>recommendations</th>\n",
       "      <th>test_case_improvements</th>\n",
       "      <th>item_id</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>actionable_recommendations</th>\n",
       "      <th>completeness.test_case_steps</th>\n",
       "      <th>completeness.data_collection</th>\n",
       "      <th>...</th>\n",
       "      <th>output</th>\n",
       "      <th>review.status</th>\n",
       "      <th>review.comments</th>\n",
       "      <th>processing_error</th>\n",
       "      <th>raw_response</th>\n",
       "      <th>testcases_summary</th>\n",
       "      <th>status</th>\n",
       "      <th>message</th>\n",
       "      <th>status_code</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complete</td>\n",
       "      <td>The test case objective is clear and specific....</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Consider adding more detailed assertions for...</td>\n",
       "      <td>['Update steps as follows: 1. Create group; ca...</td>\n",
       "      <td>P1320-TEST-2</td>\n",
       "      <td>testsuite-review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complete</td>\n",
       "      <td>The test case objective is clear and specific ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>P1320-TEST-202</td>\n",
       "      <td>testsuite-review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>complete</td>\n",
       "      <td>The test case meets its intended objective and...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>P1320-TEST-203</td>\n",
       "      <td>testsuite-review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>complete</td>\n",
       "      <td>The test case objective is clear and specific,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>P1320-TEST-204</td>\n",
       "      <td>testsuite-review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>complete</td>\n",
       "      <td>The test case objective is clear and specific....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>P1320-TEST-206</td>\n",
       "      <td>testsuite-review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  assessment_verdict                               assessment_rationale  \\\n",
       "0           complete  The test case objective is clear and specific....   \n",
       "1           complete  The test case objective is clear and specific ...   \n",
       "2           complete  The test case meets its intended objective and...   \n",
       "3           complete  The test case objective is clear and specific,...   \n",
       "4           complete  The test case objective is clear and specific....   \n",
       "\n",
       "  identified_gaps                                    recommendations  \\\n",
       "0              []  ['Consider adding more detailed assertions for...   \n",
       "1              []                                                 []   \n",
       "2              []                                                 []   \n",
       "3              []                                                 []   \n",
       "4              []                                                 []   \n",
       "\n",
       "                              test_case_improvements         item_id  \\\n",
       "0  ['Update steps as follows: 1. Create group; ca...    P1320-TEST-2   \n",
       "1                                                 []  P1320-TEST-202   \n",
       "2                                                 []  P1320-TEST-203   \n",
       "3                                                 []  P1320-TEST-204   \n",
       "4                                                 []  P1320-TEST-206   \n",
       "\n",
       "        prompt_type actionable_recommendations  completeness.test_case_steps  \\\n",
       "0  testsuite-review                        NaN                           NaN   \n",
       "1  testsuite-review                        NaN                           NaN   \n",
       "2  testsuite-review                        NaN                           NaN   \n",
       "3  testsuite-review                        NaN                           NaN   \n",
       "4  testsuite-review                        NaN                           NaN   \n",
       "\n",
       "   completeness.data_collection  ...  output  review.status  review.comments  \\\n",
       "0                           NaN  ...     NaN            NaN              NaN   \n",
       "1                           NaN  ...     NaN            NaN              NaN   \n",
       "2                           NaN  ...     NaN            NaN              NaN   \n",
       "3                           NaN  ...     NaN            NaN              NaN   \n",
       "4                           NaN  ...     NaN            NaN              NaN   \n",
       "\n",
       "   processing_error  raw_response  testcases_summary  status  message  \\\n",
       "0               NaN           NaN                NaN     NaN      NaN   \n",
       "1               NaN           NaN                NaN     NaN      NaN   \n",
       "2               NaN           NaN                NaN     NaN      NaN   \n",
       "3               NaN           NaN                NaN     NaN      NaN   \n",
       "4               NaN           NaN                NaN     NaN      NaN   \n",
       "\n",
       "   status_code  results  \n",
       "0          NaN      NaN  \n",
       "1          NaN      NaN  \n",
       "2          NaN      NaN  \n",
       "3          NaN      NaN  \n",
       "4          NaN      NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df.to_excel('all_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_traceability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
