{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f99dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            original  \\\n",
      "0  If projected the data must be readable.  On a ...   \n",
      "1  The product shall ensure that it can only be a...   \n",
      "2  All business rules specified in the Disputes S...   \n",
      "\n",
      "                                    proposed_rewrite  \n",
      "0  When data is projected on a 10x10 projection s...  \n",
      "1  The product shall allow access only to authori...  \n",
      "2  All business rules specified in the Disputes S...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import flatdict\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import pandas as pd\n",
    "import flatdict\n",
    "from pydantic import BaseModel, Field, SecretStr\n",
    "\n",
    "from src import utils\n",
    "from src.components import prompteval as pe\n",
    "from src.components.promptrunner import RateLimitOpenAIClient\n",
    "\n",
    "async def process_json_responses(\n",
    "    responses, ids, prompt_type, json_key: str = \"requirements_review\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process OpenAI responses and flatten extracted JSON structures.\"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        output = dict(response)\n",
    "        message = getattr(response.choices[0], \"message\", None)\n",
    "        if not message:\n",
    "            continue\n",
    "\n",
    "        # Parse structured JSON content if available\n",
    "        if getattr(message, \"content\", None):\n",
    "            try:\n",
    "                response_json = json.loads(message.content)\n",
    "                if json_key in response_json:\n",
    "                    nested_dicts = response_json[json_key]\n",
    "                    flat_dicts = [flatdict.FlatDict(d, delimiter=\".\") for d in nested_dicts]\n",
    "                    for d in flat_dicts:\n",
    "                        output.update(d)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                output[\"json_parse_error\"] = message.content\n",
    "\n",
    "        # Include usage info\n",
    "        if getattr(response, \"usage\", None):\n",
    "            usage = dict(response.usage)\n",
    "            usage.update(dict(getattr(response.usage, \"completion_tokens_details\", {})))\n",
    "            usage.update(dict(getattr(response.usage, \"prompt_tokens_details\", {})))\n",
    "            output.update(usage)\n",
    "\n",
    "        # Include parsed content if provided\n",
    "        if getattr(message, \"parsed\", None):\n",
    "            output.update(dict(message.parsed))\n",
    "        output.update(\n",
    "            {\n",
    "                \"requirement_id\": ids[i],\n",
    "                \"prompt_type\": prompt_type,\n",
    "            }\n",
    "        )\n",
    "        processed.append(output)\n",
    "    return processed\n",
    "\n",
    "async def run_requirement_review(\n",
    "    openai_client,\n",
    "    prompt_messages: List[str],\n",
    "    requirements: List[str],\n",
    "    ids: Optional[List[int]] = None,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    json_key: str = \"requirements_review\",\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Execute concurrent review prompts and process JSON responses.\"\"\"\n",
    "    if ids is None:\n",
    "        ids = list(range(len(requirements)))\n",
    "    # Build concurrent tasks\n",
    "    tasks = [\n",
    "        openai_client.chat_completion_parse(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_messages[0]},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_messages[1]\n",
    "                    .replace(\"{requirements}\", f\"{req_id}: {req}\")\n",
    "                    .replace(\"{enable_split}\", \"True\"),\n",
    "                },\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        for req, req_id in zip(requirements, ids)\n",
    "    ]\n",
    "    # Run all requests concurrently\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Process structured JSON responses\n",
    "    return await process_json_responses(responses, ids, prompt_messages[-1], json_key)\n",
    "\n",
    "# Instantiate the openai client and define model\n",
    "DOT_ENV = dotenv_values(\"../.env\")\n",
    "OPENAI_API_KEY = DOT_ENV['OPENAI_API_KEY']\n",
    "rl_openai_client = RateLimitOpenAIClient(api_key=OPENAI_API_KEY)\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "eval_funcs = [\n",
    "    'eval_avoids_vague_terms',\n",
    "    'eval_definite_articles_usage',\n",
    "    'eval_has_appropriate_subject_verb',\n",
    "    'eval_has_common_units_of_measure',\n",
    "    'eval_has_escape_clauses',\n",
    "    'eval_has_no_open_ended_clauses',\n",
    "    'eval_is_active_voice',\n",
    "]\n",
    "eval_weights = [\n",
    "    0.35,\n",
    "    0.05,\n",
    "    0.15,\n",
    "    0.05,\n",
    "    0.10,\n",
    "    0.10,\n",
    "    0.20\n",
    "]\n",
    "# Make eval config\n",
    "eval_config = pe.make_eval_config(pe, include_funcs=eval_funcs)\n",
    "\n",
    "# Define prompt messages\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Senior Requirements Quality Analyst and technical editor. \n",
    "You specialize in detecting and fixing requirement defects using authoritative quality rules. \n",
    "Be rigorous, consistent, and concise. Maintain the author's technical intent while removing ambiguity. \n",
    "Do not add new functionality. Ask targeted clarification questions when needed.\n",
    "\n",
    "Response Format (produce exactly this JSON structure):\n",
    "{\n",
    "  \"requirements_review\": [\n",
    "    {\n",
    "      \"requirement_id\": \"<ID>\",\n",
    "      \"original\": \"<original requirement>\",\n",
    "      \"checks\": {\n",
    "        \"R2\": {\"status\": \"pass|fail\", \"active_voice\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R3\": {\"status\": \"pass|fail\", \"appropriate_subj_verb\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R5\": {\"status\": \"pass|fail\", \"definite_articles\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R6\": {\"status\": \"pass|fail\", \"units\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R7\": {\"status\": \"pass|fail\", \"vague terms\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R8\": {\"status\": \"pass|fail\", \"escape_clauses\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R9\": {\"status\": \"pass|fail\", \"open_ended_clauses\": [\"<issues>\"], \"explanation\": \"<brief>\"}\n",
    "      },\n",
    "      \"proposed_rewrite\": \"<single improved requirement that resolves all detected issues>\",\n",
    "      \"split_recommendation\": {\n",
    "        \"needed\": true|false,\n",
    "        \"because\": \"<why>\",\n",
    "        \"split_into\": [\"<Req A>\", \"<Req B>\"]\n",
    "      },\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Evaluation method:\n",
    "1) Parse inputs and normalize IDs. \n",
    "2) For each requirement, test 2, R3, R5, R6, R7, R8, R9. \n",
    "3) Explain each failure succinctly. \n",
    "4) Rewrite to a single, verifiable sentence unless a split is recommended. \n",
    "5) Apply glossary rules for abbreviations; on first use of allowed abbreviations, prefer the expanded form with abbreviation in parentheses. \n",
    "6) If required numbers are missing and no defaults are provided, use TBD placeholders and ask explicit questions to resolve them. \n",
    "7) Summarize compliance.\n",
    "\n",
    "Important: If {requirements} is empty, respond with a single clarifying question requesting requirements to review and stop.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Task: Review and improve the following requirement statements using the provided variables.\n",
    "\n",
    "Variables:\n",
    "- Requirements (list or newline-separated; may include IDs):\n",
    "  {requirements}\n",
    "- Enable split recommendations (true|false; default true): {enable_split}\n",
    "\n",
    "Produce output strictly in the Response Format JSON. Do not use Markdown.\n",
    "\n",
    "Now perform the review on the provided inputs and return only the Response Format JSON.\n",
    "\"\"\"\n",
    "\n",
    "# Define the requirements to be revised\n",
    "requirements = [\n",
    "    \"If projected the data must be readable.  On a 10x10 projection screen  90% of viewers must be able to read Event / Activity data from a viewing distance of 30\",\n",
    "    \"The product shall ensure that it can only be accessed by authorized users.  The product will be able to distinguish between authorized and unauthorized users in all access attempts\",\n",
    "    \"All business rules specified in the Disputes System shall be in compliance to the guidelines of Regulation E and Regulation Z\",\n",
    "]\n",
    "df = pd.DataFrame({'requirements': requirements})\n",
    "\n",
    "# Run revisions and cast to dataframe\n",
    "prompt = [SYSTEM_PROMPT, USER_PROMPT]\n",
    "revisions = asyncio.run(run_requirement_review(rl_openai_client, prompt, requirements, None, MODEL))\n",
    "final_df = pd.DataFrame(revisions)\n",
    "\n",
    "# Get post-revision Accuracy Score\n",
    "final_df = pe.call_evals(final_df, col='proposed_rewrite', eval_config=eval_config)\n",
    "final_df = pe.get_failed_evals(final_df)\n",
    "pe.add_weighted_column(final_df, eval_funcs, eval_weights, \"weighted_value\")\n",
    "\n",
    "# View original and rewritten requirement statements\n",
    "print(final_df[['original', 'proposed_rewrite']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce303a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>proposed_rewrite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If projected the data must be readable.  On a ...</td>\n",
       "      <td>When data is projected on a 10x10 projection s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product shall ensure that it can only be a...</td>\n",
       "      <td>The product shall allow access only to authori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All business rules specified in the Disputes S...</td>\n",
       "      <td>All business rules specified in the Disputes S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  If projected the data must be readable.  On a ...   \n",
       "1  The product shall ensure that it can only be a...   \n",
       "2  All business rules specified in the Disputes S...   \n",
       "\n",
       "                                    proposed_rewrite  \n",
       "0  When data is projected on a 10x10 projection s...  \n",
       "1  The product shall allow access only to authori...  \n",
       "2  All business rules specified in the Disputes S...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[['original', 'proposed_rewrite']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad6eaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If projected the data must be readable.  On a 10x10 projection screen  90% of viewers must be able to read Event / Activity data from a viewing distance of 30'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['original'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347ac8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When data is projected on a 10x10 projection screen, at least 90% of viewers must be able to read the Event/Activity data from a viewing distance of 30 feet.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['proposed_rewrite'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4062d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            original  \\\n",
      "0  If projected the data must be readable.  On a ...   \n",
      "1  The product shall ensure that it can only be a...   \n",
      "2  All business rules specified in the Disputes S...   \n",
      "\n",
      "                                    proposed_rewrite  \n",
      "0  If projected, the system shall ensure that on ...  \n",
      "1  The product shall ensure that only authorized ...  \n",
      "2  The Disputes System must ensure that all busin...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import flatdict\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import pandas as pd\n",
    "import flatdict\n",
    "from pydantic import BaseModel, Field, SecretStr\n",
    "\n",
    "from src import utils\n",
    "from src.components import prompteval as pe\n",
    "from src.components.promptrunner import RateLimitOpenAIClient\n",
    "\n",
    "async def process_json_responses(\n",
    "    responses, ids, prompt_type, json_key: str = \"requirements_review\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process OpenAI responses and flatten extracted JSON structures.\"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        output = dict(response)\n",
    "        message = getattr(response.choices[0], \"message\", None)\n",
    "        if not message:\n",
    "            continue\n",
    "\n",
    "        # Parse structured JSON content if available\n",
    "        if getattr(message, \"content\", None):\n",
    "            try:\n",
    "                response_json = json.loads(message.content)\n",
    "                if json_key in response_json:\n",
    "                    nested_dicts = response_json[json_key]\n",
    "                    flat_dicts = [flatdict.FlatDict(d, delimiter=\".\") for d in nested_dicts]\n",
    "                    for d in flat_dicts:\n",
    "                        output.update(d)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                output[\"json_parse_error\"] = message.content\n",
    "\n",
    "        # Include usage info\n",
    "        if getattr(response, \"usage\", None):\n",
    "            usage = dict(response.usage)\n",
    "            usage.update(dict(getattr(response.usage, \"completion_tokens_details\", {})))\n",
    "            usage.update(dict(getattr(response.usage, \"prompt_tokens_details\", {})))\n",
    "            output.update(usage)\n",
    "\n",
    "        # Include parsed content if provided\n",
    "        if getattr(message, \"parsed\", None):\n",
    "            output.update(dict(message.parsed))\n",
    "        output.update(\n",
    "            {\n",
    "                \"requirement_id\": ids[i],\n",
    "                \"prompt_type\": prompt_type,\n",
    "            }\n",
    "        )\n",
    "        processed.append(output)\n",
    "    return processed\n",
    "\n",
    "async def run_requirement_review(\n",
    "    openai_client,\n",
    "    system_message: str,\n",
    "    user_message: str,\n",
    "    prompt_name: str,\n",
    "    requirements: List[str],\n",
    "    ids: Optional[List[int]] = None,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    json_key: str = \"requirements_review\",\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Execute concurrent review prompts and process JSON responses.\"\"\"\n",
    "    if ids is None:\n",
    "        ids = list(range(len(requirements)))\n",
    "    # Build concurrent tasks\n",
    "    tasks = [\n",
    "        openai_client.chat_completion_parse(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\",\n",
    "                    \"content\": user_message\n",
    "                    .replace(\"{requirements}\", f\"{req_id}: {req}\")\n",
    "                    .replace(\"{enable_split}\", \"True\"),\n",
    "                },\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        for req, req_id in zip(requirements, ids)\n",
    "    ]\n",
    "    # Run all requests concurrently\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Process structured JSON responses\n",
    "    return await process_json_responses(responses, ids, prompt_name, json_key)\n",
    "\n",
    "# Instantiate the openai client and define model\n",
    "DOT_ENV = dotenv_values(\"../.env\")\n",
    "OPENAI_API_KEY = DOT_ENV['OPENAI_API_KEY']\n",
    "rl_openai_client = RateLimitOpenAIClient(api_key=OPENAI_API_KEY)\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "eval_funcs = [\n",
    "    'eval_avoids_vague_terms',\n",
    "    'eval_definite_articles_usage',\n",
    "    'eval_has_appropriate_subject_verb',\n",
    "    'eval_has_common_units_of_measure',\n",
    "    'eval_has_escape_clauses',\n",
    "    'eval_has_no_open_ended_clauses',\n",
    "    'eval_is_active_voice',\n",
    "]\n",
    "eval_weights = [\n",
    "    0.35,\n",
    "    0.05,\n",
    "    0.15,\n",
    "    0.05,\n",
    "    0.10,\n",
    "    0.10,\n",
    "    0.20\n",
    "]\n",
    "# Make eval config\n",
    "eval_config = pe.make_eval_config(pe, include_funcs=eval_funcs)\n",
    "\n",
    "# Define prompt messages\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Senior Requirements Quality Analyst and technical editor. \n",
    "You specialize in detecting and fixing requirement defects using authoritative quality rules. \n",
    "Be rigorous, consistent, and concise. Maintain the author's technical intent while removing ambiguity. \n",
    "Do not add new functionality. Ask targeted clarification questions when needed.\n",
    "\n",
    "Response Format (produce exactly this JSON structure):\n",
    "{\n",
    "  \"requirements_review\": [\n",
    "    {\n",
    "      \"requirement_id\": \"<ID>\",\n",
    "      \"original\": \"<original requirement>\",\n",
    "      \"checks\": {\n",
    "        \"R2\": {\"status\": \"pass|fail\", \"active_voice\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R3\": {\"status\": \"pass|fail\", \"appropriate_subj_verb\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R5\": {\"status\": \"pass|fail\", \"definite_articles\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R6\": {\"status\": \"pass|fail\", \"units\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R7\": {\"status\": \"pass|fail\", \"vague terms\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R8\": {\"status\": \"pass|fail\", \"escape_clauses\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R9\": {\"status\": \"pass|fail\", \"open_ended_clauses\": [\"<issues>\"], \"explanation\": \"<brief>\"}\n",
    "      },\n",
    "      \"proposed_rewrite\": \"<single improved requirement that resolves all detected issues>\",\n",
    "      \"split_recommendation\": {\n",
    "        \"needed\": true|false,\n",
    "        \"because\": \"<why>\",\n",
    "        \"split_into\": [\"<Req A>\", \"<Req B>\"]\n",
    "      },\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Evaluation method:\n",
    "1) Parse inputs and normalize IDs. \n",
    "2) For each requirement, test 2, R3, R5, R6, R7, R8, R9. \n",
    "3) Explain each failure succinctly. \n",
    "4) Rewrite to a single, verifiable sentence unless a split is recommended. \n",
    "5) Apply glossary rules for abbreviations; on first use of allowed abbreviations, prefer the expanded form with abbreviation in parentheses. \n",
    "6) If required numbers are missing and no defaults are provided, use TBD placeholders and ask explicit questions to resolve them. \n",
    "7) Summarize compliance.\n",
    "\n",
    "Important: If {requirements} is empty, respond with a single clarifying question requesting requirements to review and stop.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Task: Review and improve the following requirement statements using the provided variables.\n",
    "\n",
    "Variables:\n",
    "- Requirements (list or newline-separated; may include IDs):\n",
    "  {requirements}\n",
    "- Enable split recommendations (true|false; default true): {enable_split}\n",
    "\n",
    "Produce output strictly in the Response Format JSON. Do not use Markdown.\n",
    "\n",
    "Now perform the review on the provided inputs and return only the Response Format JSON.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_NAME = 'basic-incose'\n",
    "\n",
    "# Define the requirements to be revised\n",
    "requirements = [\n",
    "    \"If projected the data must be readable.  On a 10x10 projection screen  90% of viewers must be able to read Event / Activity data from a viewing distance of 30\",\n",
    "    \"The product shall ensure that it can only be accessed by authorized users.  The product will be able to distinguish between authorized and unauthorized users in all access attempts\",\n",
    "    \"All business rules specified in the Disputes System shall be in compliance to the guidelines of Regulation E and Regulation Z\",\n",
    "]\n",
    "df = pd.DataFrame({'requirements': requirements})\n",
    "\n",
    "# Run revisions and cast to dataframe\n",
    "revisions = asyncio.run(run_requirement_review(\n",
    "    openai_client=rl_openai_client,\n",
    "    system_message=SYSTEM_PROMPT,\n",
    "    user_message=USER_PROMPT,\n",
    "    prompt_name=PROMPT_NAME,\n",
    "    requirements=requirements,\n",
    "    ids=None,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    json_key=\"requirements_review\"\n",
    "    )\n",
    ")\n",
    "final_df = pd.DataFrame(revisions)\n",
    "\n",
    "# Get post-revision Accuracy Score\n",
    "final_df = pe.call_evals(final_df, col='proposed_rewrite', eval_config=eval_config)\n",
    "final_df = pe.get_failed_evals(final_df)\n",
    "pe.add_weighted_column(final_df, eval_funcs, eval_weights, \"weighted_value\")\n",
    "\n",
    "# View original and rewritten requirement statements\n",
    "print(final_df[['original', 'proposed_rewrite']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33097344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'choices', 'created', 'model', 'object', 'service_tier',\n",
       "       'system_fingerprint', 'usage', 'completion_tokens', 'prompt_tokens',\n",
       "       'total_tokens', 'completion_tokens_details', 'prompt_tokens_details',\n",
       "       'accepted_prediction_tokens', 'audio_tokens', 'reasoning_tokens',\n",
       "       'rejected_prediction_tokens', 'cached_tokens', 'requirement_id',\n",
       "       'prompt_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95d8ff",
   "metadata": {},
   "source": [
    "This project (titled `aiswre`) seeks to integrate the best practices described in the INCOSE Guide to Writing Requirements to enhance software requirement quality using NLP and AI.\n",
    "\n",
    "### Overview of the `aiswre` project\n",
    "\n",
    "This project, `aiswre` intends to apply AI, NLP, and data science methodologies to improve the quality of software quality processes. The initial features of the project focus on using prompt engineering techniques to refine software requirements based on the rules described in section 4 of the [INCOSE Guide](https://www.incose.org/docs/default-source/working-groups/requirements-wg/gtwr/incose_rwg_gtwr_v4_040423_final_drafts.pdf?sfvrsn=5c877fc7_2). This project was inspired by the desire to enhance the field of software quality with AI and system engineering best practices. Application of LLMs bear the opportunity to advance the field of requirements engineering as initial studies have shown promising results<sup>1,2</sup>.\n",
    "\n",
    "### Design description\n",
    "\n",
    "The project will take a requirement as input, assess it against a variety of criteria, and based on the identified gaps, refine the requirement to align with rules as described in INCOSE Guide to Writing Requirements Section 4. At present, the application only leverages the input requirement and INCOSE Guide (no other information about the project) to perform the revision.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "- Set up your OpenAI API key [OPEN AI Developer quickstart](https://platform.openai.com/)\n",
    "- Add requirements dataset to the directory\n",
    "- Open a powershell terminal and enter the following to clone the repository:\n",
    "\t- `git clone https://github.com/dsobczynski88/aiswre.git <your_desired_folder_name>`\n",
    "- Navigate to the folder containing the cloned repository:\n",
    "\t- `cd <your_desired_folder_name>`\n",
    "- Create a blank `.env` file in this location and enter:\n",
    "\t- `OPENAI_API_KEY = <your_api_key>`\n",
    "- Create a virtual env:\n",
    "\t- `python -m venv venv` \n",
    "- Activate the environment (Windows Powershell):\n",
    "\t- `.\\\\venv\\Scripts\\activate.bat`\n",
    "- Enter the following commands to install the code and dependencies:\n",
    "\t- `python -m pip install -r requirements.txt`\n",
    "\t- `python -m pip install -e .`\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Prompt engineering best practices have been applied to improve the results. However at present, this work is structured in a pre-defined way that confines the workflow. The program itself may benefit from usage of more advanced approaches in AI such as langgraph flows and agent frameworks. Furthermore, the work at best is designed for a handful of INCOSE rules and therefore is still in progress. To improve the robustness and utility of this work, there are opportunities to leverage more efficient design patterns, and this too is a subject of ongoing project activities.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import flatdict\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import pandas as pd\n",
    "import flatdict\n",
    "from pydantic import BaseModel, Field, SecretStr\n",
    "\n",
    "from src import utils\n",
    "from src.components import prompteval as pe\n",
    "from src.components.promptrunner import RateLimitOpenAIClient\n",
    "\n",
    "async def process_json_responses(\n",
    "    responses, ids, prompt_type, json_key: str = \"requirements_review\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process OpenAI responses and flatten extracted JSON structures.\"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        output = dict(response)\n",
    "        message = getattr(response.choices[0], \"message\", None)\n",
    "        if not message:\n",
    "            continue\n",
    "\n",
    "        # Parse structured JSON content if available\n",
    "        if getattr(message, \"content\", None):\n",
    "            try:\n",
    "                response_json = json.loads(message.content)\n",
    "                if json_key in response_json:\n",
    "                    nested_dicts = response_json[json_key]\n",
    "                    flat_dicts = [flatdict.FlatDict(d, delimiter=\".\") for d in nested_dicts]\n",
    "                    for d in flat_dicts:\n",
    "                        output.update(d)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                output[\"json_parse_error\"] = message.content\n",
    "\n",
    "        # Include usage info\n",
    "        if getattr(response, \"usage\", None):\n",
    "            usage = dict(response.usage)\n",
    "            usage.update(dict(getattr(response.usage, \"completion_tokens_details\", {})))\n",
    "            usage.update(dict(getattr(response.usage, \"prompt_tokens_details\", {})))\n",
    "            output.update(usage)\n",
    "\n",
    "        # Include parsed content if provided\n",
    "        if getattr(message, \"parsed\", None):\n",
    "            output.update(dict(message.parsed))\n",
    "        output.update(\n",
    "            {\n",
    "                \"requirement_id\": ids[i],\n",
    "                \"prompt_type\": prompt_type,\n",
    "            }\n",
    "        )\n",
    "        processed.append(output)\n",
    "    return processed\n",
    "\n",
    "async def run_requirement_review(\n",
    "    openai_client,\n",
    "    system_message: str,\n",
    "    user_message: str,\n",
    "    prompt_name: str,\n",
    "    requirements: List[str],\n",
    "    ids: Optional[List[int]] = None,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    json_key: str = \"requirements_review\",\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Execute concurrent review prompts and process JSON responses.\"\"\"\n",
    "    if ids is None:\n",
    "        ids = list(range(len(requirements)))\n",
    "    # Build concurrent tasks\n",
    "    tasks = [\n",
    "        openai_client.chat_completion_parse(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\",\n",
    "                    \"content\": user_message\n",
    "                    .replace(\"{requirements}\", f\"{req_id}: {req}\")\n",
    "                    .replace(\"{enable_split}\", \"True\"),\n",
    "                },\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        for req, req_id in zip(requirements, ids)\n",
    "    ]\n",
    "    # Run all requests concurrently\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Process structured JSON responses\n",
    "    return await process_json_responses(responses, ids, prompt_name, json_key)\n",
    "\n",
    "# Instantiate the openai client and define model\n",
    "DOT_ENV = dotenv_values(\"../.env\")\n",
    "OPENAI_API_KEY = DOT_ENV['OPENAI_API_KEY']\n",
    "rl_openai_client = RateLimitOpenAIClient(api_key=OPENAI_API_KEY)\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "eval_funcs = [\n",
    "    'eval_avoids_vague_terms',\n",
    "    'eval_definite_articles_usage',\n",
    "    'eval_has_appropriate_subject_verb',\n",
    "    'eval_has_common_units_of_measure',\n",
    "    'eval_has_escape_clauses',\n",
    "    'eval_has_no_open_ended_clauses',\n",
    "    'eval_is_active_voice',\n",
    "]\n",
    "eval_weights = [\n",
    "    0.35,\n",
    "    0.05,\n",
    "    0.15,\n",
    "    0.05,\n",
    "    0.10,\n",
    "    0.10,\n",
    "    0.20\n",
    "]\n",
    "# Make eval config\n",
    "eval_config = pe.make_eval_config(pe, include_funcs=eval_funcs)\n",
    "\n",
    "# Define prompt messages\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Senior Requirements Quality Analyst and technical editor. \n",
    "You specialize in detecting and fixing requirement defects using authoritative quality rules. \n",
    "Be rigorous, consistent, and concise. Maintain the author's technical intent while removing ambiguity. \n",
    "Do not add new functionality. Ask targeted clarification questions when needed.\n",
    "\n",
    "Response Format (produce exactly this JSON structure):\n",
    "{\n",
    "  \"requirements_review\": [\n",
    "    {\n",
    "      \"requirement_id\": \"<ID>\",\n",
    "      \"original\": \"<original requirement>\",\n",
    "      \"checks\": {\n",
    "        \"R2\": {\"status\": \"pass|fail\", \"active_voice\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R3\": {\"status\": \"pass|fail\", \"appropriate_subj_verb\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R5\": {\"status\": \"pass|fail\", \"definite_articles\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R6\": {\"status\": \"pass|fail\", \"units\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R7\": {\"status\": \"pass|fail\", \"vague terms\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R8\": {\"status\": \"pass|fail\", \"escape_clauses\": [\"<issues>\"], \"explanation\": \"<brief>\"},\n",
    "        \"R9\": {\"status\": \"pass|fail\", \"open_ended_clauses\": [\"<issues>\"], \"explanation\": \"<brief>\"}\n",
    "      },\n",
    "      \"proposed_rewrite\": \"<single improved requirement that resolves all detected issues>\",\n",
    "      \"split_recommendation\": {\n",
    "        \"needed\": true|false,\n",
    "        \"because\": \"<why>\",\n",
    "        \"split_into\": [\"<Req A>\", \"<Req B>\"]\n",
    "      },\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Evaluation method:\n",
    "1) Parse inputs and normalize IDs. \n",
    "2) For each requirement, test 2, R3, R5, R6, R7, R8, R9. \n",
    "3) Explain each failure succinctly. \n",
    "4) Rewrite to a single, verifiable sentence unless a split is recommended. \n",
    "5) Apply glossary rules for abbreviations; on first use of allowed abbreviations, prefer the expanded form with abbreviation in parentheses. \n",
    "6) If required numbers are missing and no defaults are provided, use TBD placeholders and ask explicit questions to resolve them. \n",
    "7) Summarize compliance.\n",
    "\n",
    "Important: If {requirements} is empty, respond with a single clarifying question requesting requirements to review and stop.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Task: Review and improve the following requirement statements using the provided variables.\n",
    "\n",
    "Variables:\n",
    "- Requirements (list or newline-separated; may include IDs):\n",
    "  {requirements}\n",
    "- Enable split recommendations (true|false; default true): {enable_split}\n",
    "\n",
    "Produce output strictly in the Response Format JSON. Do not use Markdown.\n",
    "\n",
    "Now perform the review on the provided inputs and return only the Response Format JSON.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_NAME = 'basic-incose'\n",
    "\n",
    "# Define the requirements to be revised\n",
    "requirements = [\n",
    "    \"If projected the data must be readable.  On a 10x10 projection screen  90% of viewers must be able to read Event / Activity data from a viewing distance of 30\",\n",
    "    \"The product shall ensure that it can only be accessed by authorized users.  The product will be able to distinguish between authorized and unauthorized users in all access attempts\",\n",
    "    \"All business rules specified in the Disputes System shall be in compliance to the guidelines of Regulation E and Regulation Z\",\n",
    "]\n",
    "df = pd.DataFrame({'requirements': requirements})\n",
    "\n",
    "# Run revisions and cast to dataframe\n",
    "revisions = asyncio.run(run_requirement_review(\n",
    "    openai_client=rl_openai_client,\n",
    "    system_message=SYSTEM_PROMPT,\n",
    "    user_message=USER_PROMPT,\n",
    "    prompt_name=PROMPT_NAME,\n",
    "    requirements=requirements,\n",
    "    ids=None,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    json_key=\"requirements_review\"\n",
    "    )\n",
    ")\n",
    "final_df = pd.DataFrame(revisions)\n",
    "\n",
    "# Get post-revision Accuracy Score\n",
    "final_df = pe.call_evals(final_df, col='proposed_rewrite', eval_config=eval_config)\n",
    "final_df = pe.get_failed_evals(final_df)\n",
    "pe.add_weighted_column(final_df, eval_funcs, eval_weights, \"weighted_value\")\n",
    "\n",
    "# View original and rewritten requirement statements\n",
    "print(final_df[['original', 'proposed_rewrite']])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
