This project (titled `aiswre`) seeks to integrate the best practices described in the INCOSE Guide to Writing Requirements to enhance software requirement quality using NLP and AI. In this article specifically, a project using `langchain` is described which refines a given software requirement based on the rules described in Section 4 of the INCOSE Guide.

### Overview of the `aiswre` project

This project, `aiswre` intends to apply AI, NLP, and data science methodologies to improve the quality of software quality processes. The initial features of the project focus on using prompt engineering techniques to refine software requirements based on the rules described in section 4 of the [INCOSE Guide](https://www.incose.org/docs/default-source/working-groups/requirements-wg/gtwr/incose_rwg_gtwr_v4_040423_final_drafts.pdf?sfvrsn=5c877fc7_2). This project was inspired by the desire to enhance the field of software quality with AI and system engineering best practices. Application of LLMs bear the opportunity to advance the field of requirements engineering as initial studies have shown promising results<sup>1,2</sup>.

### Design description

This project leverages `langchain-openai` to build a software requirement refiner application. The project will take a requirement as input, assess it against a variety of criteria, and based on the identified gaps, will run a sequential chain of prompts to refine the requirement to align with rules as described in INCOSE Guide to Writing Requirements Section 4. At present, the application only leverages the input requirement and INCOSE Guide (no other information about the project) to perform the refinement chain. After each refinement of the requirement, each requirement is re-evaluated against the input criteria to check whether the refinement chain resulted in passing of the acceptance criteria as provided in Section 4 of the INCOSE Guide to Writing Requirements. Once all criteria have been met or the number of maximum iterations has been met, the program will stop running and provide an output file showing the input requirement, final output (refined) requirement, and the final acceptance criteria evaluation.

### Overview of Software Modules

|name         |description|
|------------ |-----------|
|aiswre       |This is the main script called when running the program. In addition to loading all configuration settings and input data, it contains the function `run_eval_loop`, which executes the requirement refinement process. Specifically, the function takes in a list of requirements, evaluates them using specific evaluation functions, and based on which criteria failed, runs tailored prompts designed to correct those specific failures. The algorithm is designed to run until all requirements have passed all criteria or the set number of maximum iterations has been met.|
|workflow     |This module is a higher level abstraction of the classes contained within the module `incose` and is called in the main program script `aiswre`. The module contains the class `BasicWorkflow` which contains the following methods: `preprocess`, `load_requirements`, `revise_requirements`, and `save_output`. The `preprocess` method instantiates the `PreprocessIncoseGuide` class to prepare the incose guide for analysis. Next, templates are built using the preprocessed guide via instantiation of the `BuildIncoseTemplates` class. To appropriately tie the templates with specific evaluation functions, the `BuildIncoseEvalConfig` class is instantiated. The method `load_requirements` loads into memory the requirements dataset (Excel file) to be analyzed. The AI review occurs in method `revise_requirements` which calls the `run_eval_loop` function. This function is preceeded and followed by calling of the `run_eval_sequence` method from the `IncoseRequirementReviewer` class. The `run_eval_sequence` is called before and after `run_eval_loop` to capture metrics associated with how well the revision performed against known evaluation criteria (based on the INCOSE Guide). Lastly, the `save_output` method collects all intermediate outputs and saves them into a dedicated run folder. The `save_output` method also writes the results to `results.xlsx` which may be thought of as a "journal" of all results generated by running the program.|
|incose       |Contains the `IncoseRequirementReviewer` class, which inherits from the base `PromptRunner` class. The `IncoseRequirementReviewer` method `revise` generates revisions of input requirements. The `revise` method takes in a list of failed evaluation function lists (`arg: eval_lists`),a list of requirement statements (`args: arg_lists`), and an optional capture function. The capture function is specified when a specific output is desired from the LLM response other than what is provided by the .content attribute. These `evals_list` and `capture_func` arguments are used to build the `RunnableSequence` objects (via the method `assemble_eval_chain_list`), which are then invoked asynchronously using the `arg_lists` as the prompt variable input. In the prompts defined here, the only input to the prompt at runtime is the requirement. The `revise` method returns the results of running the revisions for each requirement as a dataframe. This class also contains the `run_evals_sequence`, `call_evals`, and `get_failed_evals` methods. These methods operate on dataframes. The `call_evals` method evaluates a requirement statement across all functions from the `prompteval` module and if a requirement fails to meet the evaluation, a column with value 1 is generated. The `get_failed_evals` method will aggregate all columns with value 1 and return a list of the evaluation functions which failed for that particular requirement (e.g., row) of a dataframe. This module also contains the classes `BuildIncoseTemplates`, `PreprocessIncoseGuide`, and `BuildIncoseEvalConfig`. These methods are tailored to perform preprocessing and template creation specific for this project; for example, one of the methods of `BuildIncoseTemplates` is  `load_evaluation_config`, which returns a dictionary linking prompt templates (based on INCOSE Section 4 rules) with specific evaluation functions. This allows the application to know which prompts correspond to specific failed evaluations.|
|preprocess   |Contains the base `TextPreprocessor` class used to clean the extracted text. Additionally, the `Sectionalize` class is used to split PDFs into specific sections and extract specific subsections of the INCOSE Guide. This module also contains the base `BuildTemplates` class to create prompts and templates from a dataframe structure that is convenient for the purposes of this project. These classes are inherited by `PreprocessIncoseGuide`.|
|promptrunner |Contains the `PromptRunner` class that contains methods to build and run chains asynchronously. Specifically, the method `run_multiple_chains` accepts a list of chains (e.g., langchain Runnable) and a list of arguments where each element of the list is a requirement. However, prompts could be constructed with multiple arguments. . The first element in the chain arguments corresponds to the arguments of the first chain in the chains list and thus both these lists are of equal length. These chains and their respective arguments are run asynchronously when the `run_multiple_chains` method is called.|
|prompteval   |Contains functions used to evaluate requirement quality based on INCOSE Guide to Writing Requirements|
|prj_exception|Contains the `CustomException` class used to catch exceptions from project functions|
|prj_logger   |Contains the `ProjectLogger` class used to configure the logger used for the project. Additionally, the decorator `get_logs` is used to log exceptions using `CustomException` and output runtime.|
|utils    |Contains a variety of functions used to manipulate dataframes and input/output data|

**NOTE:** The folder `./src/data` houses all input and output data from the project.

### Getting started

- Set up your OpenAI API key [OPEN AI Developer quickstart](https://platform.openai.com/)
- Add requirements dataset to the directory
- Open a powershell terminal and enter the following to clone the repository:
	- `git clone https://github.com/dsobczynski88/aiswre.git <your_desired_folder_name>`
- Navigate to the folder containing the cloned repository:
	- `cd <your_desired_folder_name>`
- Create a blank `.env` file in this location and enter:
	- `OPENAI_API_KEY = <your_api_key>`
- Create a virtual env:
	- `python -m venv venv` 
- Activate the environment (Windows Powershell):
	- `.\\venv\Scripts\activate.bat`
- Enter the following commands to install the code and dependencies:
	- `python -m pip install -r requirements.txt`
	- `python -m pip install -e .`
- Fill out the config.yaml file or enter the following command to run the program:
	- `python -m aiswre -d <reqs_filename> -m <openai_model> -t <base_prompt_template> -i <max_iter>`

	|argument            |description                                                                                     |
	|--------------------|------------------------------------------------------------------------------------------------|
	|`--data` or `-d`    |The requirements dataset file name                                                              | 
	|`--model` or `-m`   |The string name of the LLM model to be used for revising the requirements                       |
	|`--template` or `-t`|The string name of the base template to be used, as defined in `config.yaml`                    | 
	|`--iternum` or `-i` |The maximum number of iterations to be run during the evaluation loop function (`run_eval_loop`)|

### Example Demo

In this example, we will evaluate five (5) requirements from the Kaggle dataset. For simplification, the extent of our evaluation will be limited to six (6) Section 4 Rules (R3,R7,R8,R9,R10,R19). The requirements to be reviewed are defined below and saved to an Excel file in the ./src/data folder. 

- <u>Define the dataset</u>.
	1. The Disputes System shall record the name of the user and the date for any activity that creates or modifies the disputes case in the system.  A detailed history of the actions taken on the case, including the date and the user that performed the action, must be maintained for auditing purposes.,
	2. The WCS system shall use appropriate nomenclature and terminology as defined by the Corporate Community Grants organization. All interfaces and reports will undergo usability tests by CCR users.,
	3.  The system will notify affected parties when changes occur affecting clinicals  including but not limited to clinical section capacity changes  and clinical section cancellations.,
	4. Application testability DESC: Test environments should be built for the application to allow testing of the application's different functions.,
	5. The product shall be platform independent. The product shall enable access to any type of development environment and platform.

- <u>Run the program.</u>
	- Now that the requirement dataset has been defined, the program will be run to evaluate these requirements. Note that the program main script (`aiswre.py`) takes in four (4) required command-line arguments. The following arguments are used for this example.
	
	|model           |template                 |iternum   |dataset                   |
	|----------------|-------------------------|----------|--------------------------|
	|gpt-4o-mini     |req-reviewer-instruct-2  |3         |src/data/demo_dataset.xlsx|

	- After executing the program, view the results in results.xlsx located in the ./src/data folder. The results.xlsx will display the metric `%_resolved_final_failed_evals`, which captures the percentage of requirements that passed all evaluation criteria post LLM-assisted revisioning.

	
- <u>Review the results.</u>
	
	
	- For the above five (5) mentioned requirements, three (3) of the five (5) were revised such that all evaluation functions passed. The original and revised requirements for these cases are presented below:
	**NOTE:** To view the revised requirement outputs, go to the generated run folder within ./src/data and open the file reqs_df_with_revisions.xlsx
	
	|Original Requirement        |Revised Requirement                                                                             |
	|----------------------------|------------------------------------------------------------------------------------------------|
	|The WCS system shall use appropriate nomenclature and terminology as defined by the Corporate Community Grants organization. All interfaces and reports will undergo usability tests by CCR users.|The WCS system shall **implement** the nomenclature defined by the Corporate Community Grants organization. All interfaces **shall complete usability tests with a minimum of 15 CCR users within a maximum time frame of 30 days following their development.**|
	|The system will notify affected parties when changes occur affecting clinicals  including but not limited to clinical section capacity changes  and clinical section cancellations.|The system shall notify affected parties when changes occur **affecting clinical section capacity. The system shall notify affected parties when changes occur affecting clinical section cancellations.**|
	|Application testability DESC: Test environments should be built for the application to allow testing of the applications different functions.|The test environments **shall** be built for the application **to test its different functions.**|
      
### Future Work

At present, this work is structured in a deterministic way that limits its ability to improvise, and the use of more complex LCEL expressions and AI agents is a future area of exploration. In addition, a more in-depth exploration of prompt engineering offers potential for the application to yield more useful results. This same thought process applies to the evaluation functions to refine the extent to which outputs can be measured. An additional area is to perform a feedback study from industry experts on the results from the tool and compare this with an AI-enabled feedback study.

The program itself will greatly benefit from usage of more advanced approaches in AI such as langgraph flows and agent frameworks. Furthermore, the work at best is designed for a handful of INCOSE rules and therefore are still in progress. To improve the robustness and utility of this work, there are opportunities to leverage more efficient design patterns, and this too is a subject of ongoing project activities.